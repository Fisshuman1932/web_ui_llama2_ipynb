{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "you can change the model if you know what you are doing. I'm to lazy to make a tutorial. just run this and if you find yourself get bored. try asking someone who can help you change the model. or maybe you can mess around like me and funny enough its work hahahah\n"
      ],
      "metadata": {
        "id": "W54aWRGQ7v7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "5wnqsZrf7obx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VZaVIF1_54B5"
      },
      "outputs": [],
      "source": [
        "# @title installing the webui\n",
        "\n",
        "import os\n",
        "\n",
        "# Data\n",
        "directory1 = \"/content/text-generation-webui\"\n",
        "\n",
        "if os.path.exists(directory1):\n",
        "  print(\"You have already installed the web ui.\")\n",
        "else:\n",
        "  !git clone https://github.com/oobabooga/text-generation-webui/\n",
        "  %cd /content/text-generation-webui\n",
        "  !pip install -r requirements.txt\n",
        "\n",
        "!mkdir /content/text-generation-webui/repositories\n",
        "%cd /content/text-generation-webui/repositories\n",
        "!git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\n",
        "!git clone https://github.com/turboderp/exllama.git\n",
        "%cd GPTQ-for-LLaMa\n",
        "!python setup_cuda.py install\n",
        "%cd /content/text-generation-webui/repositories/exllama\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "print(\"###########################################\")\n",
        "print(\"This cell is done, move on to the next one.\")\n",
        "print(\"###########################################\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title installing the model. Llama-2-7b-chat-gptq\n",
        "#@markdown IMPORTANT: If the machine throws an exception like \"Exception: Could not start cloudflared.\" and refuses to give the streaming api link, stop the cell and run it again. WITHOUT disconnecting the colab entirely.\n",
        "\n",
        "\n",
        "#@markdown after generating the API. click the gradio live link and load the model using model loader GPTQ-forLLaMa and then load. you can already chatting but if you use sillytavern or tavern ai. you can use it via copy paste your link that this notebook give you\n",
        "\n",
        "\n",
        "#@markdown you will see these link https://november-record-played.trycloudflare.com/api and wss://choosing-warnings-room.trycloudflare.com/api/v1/stream\n",
        "\n",
        "#@markdown https one usually that light up blue. place it on blocking API url and then the wss goes to the streaming API url.\n",
        "%cd /content\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements.txt\n",
        "!pip install -U gradio==3.28.3\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-Chat-GPTQ/raw/main/config.json -d /content/text-generation-webui/models/Llama-2-7b-Chat-GPTQ -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-Chat-GPTQ/raw/main/generation_config.json -d /content/text-generation-webui/models/Llama-2-7b-Chat-GPTQ -o generation_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-Chat-GPTQ/raw/main/special_tokens_map.json -d /content/text-generation-webui/models/Llama-2-7b-Chat-GPTQ -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-Chat-GPTQ/resolve/main/tokenizer.model -d /content/text-generation-webui/models/Llama-2-7b-Chat-GPTQ -o tokenizer.model\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-Chat-GPTQ/raw/main/tokenizer_config.json -d /content/text-generation-webui/models/Llama-2-7b-Chat-GPTQ -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-Chat-GPTQ/resolve/main/gptq_model-4bit-128g.safetensors -d /content/text-generation-webui/models/Llama-2-7b-Chat-GPTQ -o gptq_model-4bit-128g.safetensors\n",
        "\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/text-generation-webui/extensions/api\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/text-generation-webui/\n",
        "%cd /content/text-generation-webui\n",
        "!python server.py --share --chat --wbits 4 --groupsize 128 --model_type llama --api --public-api"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qysROT9m6Ecc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}